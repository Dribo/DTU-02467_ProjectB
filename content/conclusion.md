---
title: Conclusion
prev: text-analysis
---

In this work we successfully found structure in a vaguely labelled giant of knowledge known as wikipedia. 
Most of Wikipedia knowledge is connected through references and not tags or labels, a problem which makes text 
analysis non-trivial.

We have successfully used networks as the glue needed for a meaningful analysis of scattered wikipedia pages. 
The TF-IDF analysis and wordcloud representations of communities meets or even exceeds our expectations of how much 
meaning can be extracted with what is comparatively simple and quick methods.

We think we have only scratched the surface of all the possibilities in this work. In terms of categorization, ChatGPT 
or other Machine Learning tools could be used, which would likely improve greatly upon our 'event, person or neither' 
approach.

For the purpose of answering our initial question of whether there is bias between for example the eastern and 
western block, this is not something that came to surface in our text analysis. There is no reason to rule out this 
option, as we only looked on few words and a subset of our initial data. 

Further work could explore the possibility of scraping more than one layer of wikipedia. Another course of action could
be to look at sentiment analysis in order to discover whether some articles contain a bias in the use of language. It
might be that there is a notable difference in the way events are described if for instance the US instigated a conflict
compared to if it was the Soviet Union. 

Another approach could be to look at articles that are written in multiple languages. Is there for instance a difference
in how events are described in German compared to English. 

These are just some examples of the possibilities with this data, and there are certainly many ways to expand upon the
work that we have done.