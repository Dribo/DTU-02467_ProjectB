---
title: Conclusion
prev: text-analysis
---

In this work we successfully found structure in a vaguely labelled giant of knowledge known as wikipedia. Most of Wikipedia knowledge is connected through references and not tags or labels, a problem which makes text analysis non-trivial.

We have successfully used networks as the glue needed for a meaningful analysis of scattered wikipedia pages. The TK-IDF analysis and wordcloud representations of communities meets or even exceeds our expectations of how much meaning can be extracted with what is comparatively simple and quick methods.

We think we have only scratched the possibilities in this work. In terms of categorization, ChatGPT or other Machine Learning tools could be used, which would likely improve greatly upon our 'event, person or neither' approach.

For the purpose of answering our initial question of whether there is bias between for example the eastern and western block, this is not something that came to surface in our text analysis. There is no reason to rule out this option, as we only looked on few words and a subset of our initial data. Further work could explore the possibility of scraping more than one layer of wikipedia.
